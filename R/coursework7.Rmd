---
title: "Coursework7"
author: "Fortunat Mutunda"
date: "March 23, 2016"
output: pdf_document
---
```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(arules)
library(arulesViz)
library(ggplot2)
library(TraMineR)
library(vcd)
```
1. Report which tools you decided to use, how you used them, what were the first results.
Also report the running times for the tools chosen.


***comments:***
*Mine rules from the supermarket txt*
1. Run whatever tool you run and plot the running time 
2. Get support and confidence
3. Which tool did you use , how did you use it how long did it run.
4. Discribe in general... show examples, max, min,... etc.
5. How did you get the result what params etc...


For this first tast I have selected the *arules* library because that's the one I am used to,
and it's the one I've been using for the couple past homeworks. It is not only easy to use but it is also very intuitive and pretty straight forward. With its nice inuilt function `read.transaction()` I can get all the transations without an hassle.

```{r, message=FALSE, fig.retina=T}
#df = read.csv("supermarket.txt", header = F, sep = " ")

#Starts from here
ptm <- proc.time()
trans = read.transactions("supermarket.txt", format = "basket" , sep = " ")
itemFrequencyPlot(trans, support = 0.05)
```


1.  First I've read the transaction then I have proceeded by getting which items are the most frequents in the transaction. And it appears that `9108` is the most frequent item in the given itemset, when I use a support of 0.05.


```{r, message=FALSE, fig.retina=T}
#rules
ruless <- apriori(trans, parameter= list(supp=0.005))
plot(ruless, method = "graph")
plot(ruless, method="paracoord", control=list(reorder=TRUE))

```


* Second I have applied the `apriori` alogrithm on the given transaction with a support of 0.05. And the I have plotted two differents plots.
*Notice* that whenever a client almost get any kind of item they also end up getting the item `5330` but bigger chances are mostly when `12562` is gotten then `5330` will be gotten too or when `14914` it is still the same case.


```confidence  0.8   |   minval 0.1  | smax 1  | arem none | aval FALSE   | originalSupport TRUE | support  0.005 | minlen 1 | maxlen 10```




* This is the running time of the whole process.

```{r}
proc.time() - ptm
```
`proc.time` returns five elements for backwards compatibility, but its print method prints a named vector of length 3. The first two entries are the total user and system CPU times of the current R process and any child processes on which it has waited, and the third entry is the realâ€™ elapsed time since the process was started.

2. Report overall 5 different high-support, high-confidence, high-lift rules; provide the respective contingency tables and scores.
***contengancy table ***
if I can find a tool that get me a contengancy table.
show the top 5 rules.


The code here might be a bit redundant but I am going to try to get first the top five 5 support


```{r,fig.retina=T}

ruless.sorted = sort(ruless,by = "support")
five_rules = head(ruless.sorted,5)
inspect(five_rules)
```

Then the top 5 hight confidence 

```{r,fig.retina=T}

ruless.sorted = sort(ruless,by = "confidence")
five_rules = head(ruless.sorted,5)
inspect(five_rules)
```

Lastly the top 5 lift

```{r,fig.retina=T}

ruless.sorted = sort(ruless,by = "lift")
five_rules = head(ruless.sorted,5)
inspect(five_rules)
```





3. Discuss whether some other scores studied last week or in the lecture slides would help identify "more interesting" and different rules?


***Use something from last homewok***
no confidence or lift or anything try other maesures 

```{r}
interestMeasure(ruless,c("oddsRatio", "leverage"))

```

4. Given the ability to discover frequent itemsets and association rules, propose a strategy to use these tools to study different customer segments, shops, shopping times, or specific products.
***What can you do with the data?***
Just write something, what method toools you have learnt so far to study diff rules...
Suggest a plan...etc.
There is nothing specific, just be creative... In which part can you use frequent set item or item set mining 

5. Select some relatively high-support high-confidence rule (A->B) and based on that example describe the conditional probabilities P(A|B) and P(B|A), as well as the Bayes rule.

*** Just learn what a conditional probalities ***

6. (Bonus 2p) Run Krimp on same data, provide commands and describe your findings and compare to Frequent Itemset Mining + Association rules. (link to Krimp documentation)

***Krimp ***

```{r, echo=FALSE}
#> install.packages("~/Desktop/KrimpSourceBin20130201_win,lin.zip", repos = NULL, type = "win.binary")
#Error in install.packages : cannot install Windows binary packages on this platform
```


